I"À<h2 id="overview">Overview</h2>

<p>Sighted users have access to a screenâ€™s worth of information when interacting with computers and mobile devices. Visually Challenged users access the same information primarily via screen readers and physical braille displays, while users with low vision can make use of visual content by means of screen magnifiers. Screen readers present information in the form of synthesized speech sequentially and upon command.</p>

<p>In its simplest form, this project explores the following question:</p>

<p><strong><em>What if visually challenged users were presented with more auditory information to access at a time?</em></strong></p>

<p>Over the course of the project, these questions have become a lot more defined, and have grounded themselves in real problems and past studies in the area.</p>

<p>In parallel, I have been motivated by the idea that this project, and the basic research that it attempts, might eventually help in the design of more ambitious auditory interfaces. Perhaps of the kind where information is sonified all around us, and can be accessed by turning our heads or pointing a finger, as naturally as we might glance over a screen or display today.</p>
:ET